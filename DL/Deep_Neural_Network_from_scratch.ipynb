{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing packages\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Neurons\n",
    "nn_architecture = [\n",
    "    {\"layer_size\": 4, \"activation\": \"none\"},\n",
    "    {\"layer_size\": 5, \"activation\": \"relu\"},\n",
    "    {\"layer_size\": 4, \"activation\": \"relu\"},\n",
    "    {\"layer_size\": 3, \"activation\": \"relu\"},\n",
    "    {\"layer_size\": 1, \"activation\": \"sigmoid\"}\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_parameters(nn_architecture, seed=3):\n",
    "    np.random.seed(seed)\n",
    "    \n",
    "    # parameters stores weights and biases in form of dictionary\n",
    "    parameters = {}\n",
    "    number_of_layers = len(nn_architecture)\n",
    "    for l in range(1, number_of_layers):\n",
    "        parameters['W'+ str(l)] = np.random.randn(\n",
    "            nn_architecture[l][\"layer_size\"],\n",
    "            nn_architecture[l-1][\"layer_size\"]\n",
    "        )*0.01\n",
    "        \n",
    "        parameters[\"b\"+str(l)] = np.zeros((nn_architecture[l][\"layer_size\"],1))\n",
    "        \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Activation Functions\n",
    "\n",
    "def sigmoid(Z):\n",
    "    return 1/(1 + np.exp(-Z))\n",
    "\n",
    "def relu(Z):\n",
    "    return np.maximum(0, Z)\n",
    "\n",
    "def sigmoid_backward(dA, Z):\n",
    "    S = sigmoid(Z)\n",
    "    dS = S*(1-S)\n",
    "    return dA*dS\n",
    "\n",
    "def relu_backward(dA, Z):\n",
    "    dZ = np.array(dA, copy=True)\n",
    "    dZ[Z<=0] = 0\n",
    "    return dZ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Forward function for L-Model\n",
    "\n",
    "def L_model_forward(X, parameters, nn_architecture):\n",
    "    \n",
    "    forward_cache = {}\n",
    "    number_of_layers = len(nn_architecture)\n",
    "    A = X\n",
    "    \n",
    "    for l in range(1, number_of_layers):\n",
    "        A_prev = A\n",
    "        W = parameters[\"W\"+str(l)]\n",
    "        b = parameters[\"b\"+str(l)]\n",
    "        \n",
    "        activation = nn_architecture[l][\"activation\"]\n",
    "        \n",
    "        Z, A = linear_activation_forward(A_prev, W, b, activation)\n",
    "        forward_cache['Z'+str(l)] = Z\n",
    "        forward_cache['A'+str(l)] = A\n",
    "        \n",
    "    AL = A\n",
    "    return AL, forward_cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_activation_forward(A_prev, W, b, activation):\n",
    "    Z = linear_forward(A_prev, W, b)\n",
    "    \n",
    "    if activation == \"sigmoid\":\n",
    "        A = sigmoid(Z)\n",
    "    elif activation == \"relu\":\n",
    "        A = relu(Z)\n",
    "        \n",
    "    return Z, A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_forward(A, W, b):\n",
    "    return np.dot(W, A) + b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_cost(AL, Y):\n",
    "    m = Y.shape[1]\n",
    "    \n",
    "    loprobs = np.multiply(np.log(AL), Y) + np.multiply(1-np.log(AL), 1-Y)\n",
    "    \n",
    "    cost = np.sum(logprobs)/m\n",
    "    \n",
    "    cost = np.squeeze(cost)\n",
    "    \n",
    "    return cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def L_model_backward(AL, Y, parameters, forward_cache, nn_architecture):\n",
    "    grads = {}\n",
    "    number_of_layers = len(nn_architecture)\n",
    "    m = AL.shape[1]\n",
    "    Y = Y.reshape(AL.shape) \n",
    "    \n",
    "    #Initialisation of bacpropagation\n",
    "    \n",
    "    dAL = -(np.divide(Y, AL) - np.divide(1-Y, 1-AL))\n",
    "    dA_prev = dAL\n",
    "    \n",
    "    for l in reversed(range(1, number_of_layers)):\n",
    "        \n",
    "        dA_curr = dA_prev\n",
    "        \n",
    "        activation = nn_architecture[l][\"activation\"]\n",
    "        W_curr = parameters['W' + str(l)]\n",
    "        Z_curr = forward_cache['Z' + str(l)]\n",
    "        A_prev = forward_cache['A' + str(l-1)]\n",
    "        \n",
    "        dA_prev, dW_curr, db_curr = linear_activation_backward(dA_curr, Z_curr, \n",
    "                                                               A_prev, W_curr, activation)\n",
    "        \n",
    "        grads[\"dW\"+str(l)] = dW_curr\n",
    "        grads[\"db\"+str(l)] = db_curr\n",
    "    return grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_activation_backwards(dA, Z, A_prev, W, activation):\n",
    "    \n",
    "    if activation==\"relu\":\n",
    "        dZ = relu_backward(dA, Z)\n",
    "        dA_prev, dW, db = linear_backward(dZ, A_prev, W)\n",
    "        \n",
    "    elif activation==\"sigmoid\":\n",
    "        dZ = sigmoid_backward(dA, Z)\n",
    "        dA_prev, dW, db = linear_backward(dZ, A_prev, W)\n",
    "        \n",
    "    return dA_prev, dW, db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_backward(dZ, A_prev, W):\n",
    "    \n",
    "    m = A_prev.shape[1]\n",
    "    \n",
    "    dW = np.dot(dZ, A_prev.T)\n",
    "    db = np.sum(dZ, axis=1, keepdims=True)/m\n",
    "    dA_prev = np.dot(W.T, dZ)\n",
    "    \n",
    "    return dA_prev, dW, db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_parameters(parameters, grads, learning_rate):\n",
    "    \n",
    "    L = len(parameters)\n",
    "    \n",
    "    for l in range(1, L):\n",
    "        parameters['W'+str(l)] -= learning_rate*grads[\"dW\"+str(l)]\n",
    "        parameters['b'+str(l)] -= learning_rate*grads[\"db\"+str(l)]\n",
    "        \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def L_layer_model(X, Y, nn_architecture, learning_rate = 0.0075, num_iterations = 3000, print_cost=False):\n",
    "    np.random.seed(1)\n",
    "    # keep track of cost\n",
    "    costs = []\n",
    "    \n",
    "    # Parameters initialization.\n",
    "    parameters = initialize_parameters(nn_architecture)\n",
    "    \n",
    "    # Loop (gradient descent)\n",
    "    for i in range(0, num_iterations):\n",
    "\n",
    "        # Forward propagation: [LINEAR -> RELU]*(L-1) -> LINEAR -> SIGMOID.\n",
    "        AL, forward_cache = L_model_forward(X, parameters, nn_architecture)\n",
    "        \n",
    "        # Compute cost.\n",
    "        cost = compute_cost(AL, Y)\n",
    "    \n",
    "        # Backward propagation.\n",
    "        grads = L_model_backward(AL, Y, parameters, forward_cache, nn_architecture)\n",
    " \n",
    "        # Update parameters.\n",
    "        parameters = update_parameters(parameters, grads, learning_rate)\n",
    "                \n",
    "        # Print the cost every 100 training example\n",
    "        if print_cost and i % 100 == 0:\n",
    "            print(\"Cost after iteration %i: %f\" %(i, cost))\n",
    "\n",
    "        costs.append(cost)\n",
    "            \n",
    "    # plot the cost\n",
    "    plt.plot(np.squeeze(costs))\n",
    "    plt.ylabel('cost')\n",
    "    plt.xlabel('iterations (per tens)')\n",
    "    plt.title(\"Learning rate =\" + str(learning_rate))\n",
    "    plt.show()\n",
    "    \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'lr_utils'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-15-05f9803d8eb1>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mPIL\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mImage\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mscipy\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mndimage\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mlr_utils\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mload_dataset\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun_line_magic\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'matplotlib'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'inline'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'lr_utils'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import h5py\n",
    "import scipy\n",
    "from PIL import Image\n",
    "from scipy import ndimage\n",
    "from lr_utils import load_dataset\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
